{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b76927-344a-43ab-b401-f2b0908502af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder \n",
    "from mlxtend.frequent_patterns import apriori, association_rules \n",
    "\n",
    "import cv2 \n",
    "import dlib \n",
    "import face_recognition \n",
    "from mtcnn.mtcnn import MTCNN \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import glob \n",
    "from collections import defaultdict, Counter \n",
    "import os \n",
    "from PIL import Image \n",
    "from numpy import savez_compressed \n",
    "from numpy import asarray \n",
    "from os import listdir \n",
    "import shutil \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from IPython.display import display_html \n",
    "from IPython.display import display, HTML \n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.patches import Rectangle \n",
    "from matplotlib.patches import Circle \n",
    "\n",
    "import re \n",
    "from itertools import combinations \n",
    "from ast import literal_eval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed710c94-baa4-4dfc-8d65-77ada2ae4708",
   "metadata": {},
   "source": [
    "This code utilizes various libraries for data processing, machine learning, and computer vision tasks. Libraries like mlxtend are used for pattern mining and association rule learning, while cv2 (OpenCV), dlib, face_recognition, and MTCNN provide robust tools for face detection and recognition. NumPy and pandas handle numerical computations and data manipulation, respectively. Modules like glob, os, and shutil facilitate file and directory management, and Pillow (PIL) is used for image processing. Scikit-learn supports machine learning model evaluation, while matplotlib aids in data visualization. The code also uses modules such as re for regular expressions, itertools for combinations, and ast for safe evaluation of Python literals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295b81e-56ec-4453-a0cd-0a8f0a1f6301",
   "metadata": {},
   "source": [
    "## EXTRACT DATA USING FACE RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d9186-ede3-4f4f-b319-5551a8127e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = f'./known_embeddings_{pengguna_user}'\n",
    "os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "def save_embedding(person_id, face_encoding):\n",
    "    number = person_id.split('_')[-1]\n",
    "    embedding_path = os.path.join(embedding_dir, f'person_{number}.npy')\n",
    "    np.save(embedding_path, face_encoding)\n",
    "\n",
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    for file in glob.glob(os.path.join(embedding_dir, '*.npy')):\n",
    "        person_id = os.path.splitext(os.path.basename(file))[0]\n",
    "        embeddings[person_id] = np.load(file)\n",
    "    return embeddings\n",
    "\n",
    "def find_best_match(face_encoding, known_embeddings):\n",
    "    best_match_id = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for person_id, known_encoding in known_embeddings.items():\n",
    "        distance = np.linalg.norm(face_encoding - known_encoding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            number = person_id.split('_')[-1]\n",
    "            best_match_id = number\n",
    "    \n",
    "    return best_match_id if min_distance < 0.6 else None  # Adjust threshold as needed\n",
    "\n",
    "def process_image(image_path, output_base_dir, known_embeddings, photo_id):\n",
    "    image = cv2.imread(image_path)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "    \n",
    "    person_ids = []\n",
    "    person_names = []\n",
    "\n",
    "    for i, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "        top, right, bottom, left = face_location\n",
    "        face_image = image[top:bottom, left:right]\n",
    "\n",
    "        person_id = find_best_match(face_encoding, known_embeddings)\n",
    "\n",
    "        if not person_id:\n",
    "            person_id = f'{len(known_embeddings) + 1}'\n",
    "            save_embedding(person_id, face_encoding)\n",
    "            known_embeddings[person_id] = face_encoding\n",
    "            \n",
    "        person_dir = os.path.join(output_base_dir, f'person_{person_id}')\n",
    "        os.makedirs(person_dir, exist_ok=True)\n",
    "\n",
    "        face_index = len(glob.glob(os.path.join(person_dir, \"*.jpg\"))) + 1\n",
    "        face_filename = os.path.join(person_dir, f'person_{person_id}_face_{face_index}.jpg') \n",
    "        cv2.imwrite(face_filename, face_image)\n",
    "\n",
    "        person_face_id = f\"{person_id}_{face_index}\"\n",
    "\n",
    "        person_ids.append(person_face_id)\n",
    "        person_names.append(f'person_{person_id}_face_{face_index}')  # Updated naming\n",
    "\n",
    "    return {\n",
    "        'photo_id': photo_id,\n",
    "        'photo_name': os.path.basename(image_path),\n",
    "        'person_id': ','.join(person_ids),\n",
    "        'person': person_names\n",
    "    }\n",
    "\n",
    "def process_images(image_directory, output_base_dir, existing_dataset_path):\n",
    "    image_paths = glob.glob(os.path.join(image_directory, '*.jpg'))\n",
    "    \n",
    "    if os.path.exists(existing_dataset_path):\n",
    "        existing_df = pd.read_csv(existing_dataset_path)\n",
    "        processed_photos = set(existing_df['photo_name'])\n",
    "        start_photo_id = existing_df['photo_id'].max() + 1\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        processed_photos = set()\n",
    "        start_photo_id = 1\n",
    "    \n",
    "    dataset = []\n",
    "    known_embeddings = load_embeddings()\n",
    "    current_photo_id = start_photo_id\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        photo_name = os.path.basename(image_path)\n",
    "        if photo_name in processed_photos:\n",
    "            continue\n",
    "\n",
    "        result = process_image(image_path, output_base_dir, known_embeddings, current_photo_id)\n",
    "        dataset.append(result)\n",
    "        current_photo_id += 1\n",
    "\n",
    "    new_df = pd.DataFrame(dataset)\n",
    "    combined_df = pd.concat([existing_df, new_df])\n",
    "    if new_df.empty:\n",
    "        combined_df.to_csv(existing_dataset_path, index=False)\n",
    "        print(\"tidak ada tambahan foto baru\")\n",
    "    else:\n",
    "        combined_df.to_csv(existing_dataset_path, index=False)\n",
    "        print(f\"ada tambahan foto baru sebanyak {len(new_df)} foto, data foto menjadi {len(combined_df)}\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998daf5c-cd52-41d5-a1e0-ce97e56a6cab",
   "metadata": {},
   "source": [
    "This code handles face detection, recognition, and dataset management for images. It uses pre-trained face encodings to identify known faces or assign new IDs to unknown ones by comparing face encodings and saving new embeddings when needed. Detected faces are cropped, saved in corresponding directories, and named with unique identifiers. The code processes all images in a specified directory, skipping already-processed ones, and updates a dataset CSV file with information about the new images and their detected faces, ensuring that all data is up-to-date and consistent with the current state of the image repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b7d1c2-c3f2-49a1-b342-aa146b9ed081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pengguna_user = '( GALLERY_NAME )' #PLEASE CHANGE STRING NAME TO YOUR ACTUAL GALLERY NAME IN THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c66c53-02cf-4698-a75c-d3633278dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = f'./{pengguna_user}'\n",
    "output_base_dir = f'./clusterExtract/{pengguna_user}'\n",
    "existing_dataset_path = f'./face_dataset_{pengguna_user}_before_koreksi_sistem.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef046a5-eaf8-40ed-a72e-1fef3414f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process = process_images(image_dir, output_base_dir, existing_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e4206-e266-4798-9566-cd8a882cdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct =  df_process.dropna()\n",
    "df_correct = df_correct[df_correct['person'] != '[]']\n",
    "df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa18b28-7589-43cb-b3d8-051091d8085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct.to_csv(existing_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2a448-66c8-4511-a5c0-473843f5ee37",
   "metadata": {},
   "source": [
    "To use this code, follow these steps:\n",
    "\n",
    "1. **Set User Parameters**: Define the user identifier (`pengguna_user`) to specify which dataset and directories to use.\n",
    "2. **Define Directories and Paths**: Set the `image_dir` for input images, `output_base_dir` for saving processed images, and `existing_dataset_path` for the CSV file that tracks the face data.\n",
    "3. **Run the Processing Function**: Call `process_images(image_dir, output_base_dir, existing_dataset_path)` to detect and recognize faces in the specified directory, save them in organized folders, and update the dataset CSV.\n",
    "4. **Clean the Dataset**: Filter the processed DataFrame (`df_process`) to remove empty or invalid entries, then save the cleaned dataset back to the CSV file.\n",
    "\n",
    "By following these steps, you will maintain an updated dataset of recognized faces and newly detected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cae88e-329a-449a-9da4-66b5d93ae311",
   "metadata": {},
   "source": [
    "## CORRECTION TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ade95f-d25e-4b1a-b01e-9522c39ffa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_face_index(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    \n",
    "    face_files = glob.glob(os.path.join(directory, \"*.jpg\"))\n",
    "    \n",
    "    if not face_files:\n",
    "        return 0\n",
    "    \n",
    "    expected_prefix = os.path.basename(directory).split('_')[-1]\n",
    "    \n",
    "    indices = []\n",
    "    for face in face_files:\n",
    "        filename = os.path.basename(face)\n",
    "        \n",
    "        if filename.startswith(f\"person_{expected_prefix}\"):\n",
    "            match = re.search(r'face_(\\d+)', filename)\n",
    "            if match:\n",
    "                indices.append(int(match.group(1)))\n",
    "    \n",
    "    return max(indices, default=0)\n",
    "\n",
    "\n",
    "def rename_face_images(output_base_dir, old_name, new_name):\n",
    "    old_person_id = old_name.split('_')[1]\n",
    "    new_person_id = new_name.split('_')[1]\n",
    "\n",
    "    old_path = os.path.join(output_base_dir, f'person_{new_person_id}', f\"{old_name}.jpg\")\n",
    "    new_path = os.path.join(output_base_dir, f'person_{new_person_id}', f\"{new_name}.jpg\")\n",
    "\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "\n",
    "def check_folder_consistency(output_base_dir, df):\n",
    "    print(\"Checking folder consistency...\")\n",
    "    count = 0\n",
    "\n",
    "    for person_folder in glob.glob(os.path.join(output_base_dir, 'person_*')):\n",
    "        dir_folder = person_folder\n",
    "        person_id = os.path.basename(person_folder).split('_')[-1]\n",
    "        print(f\"Checking folder: {person_folder}\")\n",
    "\n",
    "        for face_image in glob.glob(os.path.join(person_folder, '*.jpg')):\n",
    "            face_name = os.path.basename(face_image)\n",
    "            base_name = face_name.split('_')[0]\n",
    "            person_id_name = face_name.split('_')[1]\n",
    "            face_name = f\"{base_name}_{person_id_name}\"\n",
    "            expected_person_id = f'person_{person_id}'\n",
    "\n",
    "            if face_name != expected_person_id:\n",
    "                old_person_id = face_name.split('_')[1]\n",
    "                old_person_id_2 = face_image.split('_')[4][:-4]\n",
    "                old_img_id = f'{old_person_id}_{old_person_id_2}'\n",
    "                print(f\"File person_{old_person_id} does not match expected person_id {expected_person_id}\")\n",
    "                expected_person_id_2 = get_highest_face_index(dir_folder) + 1\n",
    "                new_img_id = f'{person_id}_{expected_person_id_2}'\n",
    "\n",
    "                def match_person_id(row, old_img_id):\n",
    "                    return old_img_id in row['person_id'].split(',')\n",
    "\n",
    "                matching_rows = df[df.apply(lambda row: match_person_id(row, old_img_id), axis=1)]\n",
    "                if not matching_rows.empty:\n",
    "                    old_person_name = f'{face_name}_face_{old_person_id_2}'\n",
    "                    new_person_name = f'{expected_person_id}_face_{expected_person_id_2}'\n",
    "                    rename_face_images(output_base_dir, old_person_name, new_person_name)\n",
    "\n",
    "                    for index, row in matching_rows.iterrows():\n",
    "                        person_list = eval(row['person']) \n",
    "                        person_id_list = row['person_id'].split(',')\n",
    "                        updated_person_list = [new_person_name if p == old_person_name else p for p in person_list]\n",
    "                        updated_person_id_list = [new_img_id if p == old_img_id else p for p in person_id_list]\n",
    "                        update_df_id = ','.join(updated_person_id_list)\n",
    "                        df.at[index, 'person'] = str(updated_person_list)\n",
    "                        df.at[index, 'person_id'] = str(update_df_id)\n",
    "                        print(f\"Updated entry: {old_person_name} -> {new_person_name}\")\n",
    "                        print(f\"From photo_id: {matching_rows['photo_id'].to_string(index=False)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044ef08-71a4-4c73-9bc2-bdf35ac2f52f",
   "metadata": {},
   "source": [
    "This code ensures consistency between folder names and image file names in a face recognition dataset. It checks each person's folder to verify that image filenames match their corresponding directory and renames any mismatched files to maintain proper formatting. It updates the associated DataFrame by replacing old IDs and names with corrected ones, ensuring all data entries accurately reflect the current state of the images and folders. The process helps maintain a clean and organized dataset for reliable face recognition and identification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943d328-e2e5-4af5-a17c-c637331c5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = pd.read_csv(f'./face_dataset_{pengguna_user}_before_koreksi_sistem.csv')\n",
    "df_correct_temp = df_correct.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51e590-cb5e-4832-98e4-23daca890a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct_after = check_folder_consistency(output_base_dir, df_correct_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f15806b-ece4-412a-b32b-4a8bc5707a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct_after.to_csv(f'./face_dataset_{pengguna_user}_after_koreksi_sistem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a7cb8-6d76-4f57-a6a8-0c1e9d05d5a7",
   "metadata": {},
   "source": [
    "To use this code:\n",
    "\n",
    "1. **Prepare Face Clusters**: Before executing the code, ensure that face clusters are correctly assigned to the corresponding person folders. For instance, if you find faces in the `person_1` folder that actually belong to `person_13`, move those images to the `person_13` folder. Perform this check across all folders. This process is time-consuming and requires consistent effort but is crucial for accurate results due to the current limitations of face detection methods.\n",
    "\n",
    "2. **Load the Initial Dataset**: Load the dataset (`face_dataset_{pengguna_user}_before_koreksi_sistem.csv`) into a DataFrame.\n",
    "\n",
    "3. **Check and Correct Folder Consistency**: Run the `check_folder_consistency` function to verify that face image filenames match their corresponding person folders. This function will rename mismatched files and update the DataFrame accordingly.\n",
    "\n",
    "4. **Save the Corrected Dataset**: Save the updated DataFrame to a new CSV file (`face_dataset{pengguna_user}_after_koreksi_sistem.csv`) to ensure that the dataset reflects all corrections.\n",
    "\n",
    "By following these steps, the dataset will remain consistent and accurate for face recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a3c09-1fc6-438b-aebb-3792d9dd0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def count_person_2_face(person_list_str):\n",
    "    person_list = ast.literal_eval(person_list_str)\n",
    "    return sum(bool(re.match(r'person_2_face_\\d+', person)) for person in person_list)\n",
    "\n",
    "total_count_before = df_correct['person'].apply(count_person_2_face).sum()\n",
    "total_count_after = df_correct_after['person'].apply(count_person_2_face).sum()\n",
    "\n",
    "print(f'JUMLAH Before: {total_count_before}')\n",
    "print(f'JUMLAH After: {total_count_after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e387d8e-169d-46a6-9fe0-912f7a5df3c0",
   "metadata": {},
   "source": [
    "This code compares the number of occurrences of `person_2_face` in the dataset before and after running the consistency check and correction.\n",
    "\n",
    "1. **Define a Counting Function**: `count_person_2_face` parses the `person` column entries to count how many times a pattern like `person_2_face_x` appears.\n",
    "2. **Calculate Total Counts**: It calculates the total count of `person_2_face` entries before (`df_correct`) and after (`df_correct_after`) the consistency check.\n",
    "3. **Display Results**: The results are printed to show the differences, helping to confirm whether the data corrections were applied properly.\n",
    "\n",
    "**Example Output**:\n",
    "```\n",
    "Before: 15\n",
    "After: 13\n",
    "```\n",
    "\n",
    "This output indicates that there were 15 instances of `person_2_face` before corrections and 13 after, suggesting some inconsistencies were resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d253ce-107e-4621-b78e-ca8c16303174",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff52bfc8-8682-49b9-907f-6ad4f28864a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_second_underscore(person_list):\n",
    "    modified_list = []\n",
    "    for person in person_list:\n",
    "        parts = person.split('_')\n",
    "        if len(parts) > 2:\n",
    "            modified_list.append('_'.join(parts[:2])) \n",
    "        else:\n",
    "            modified_list.append(person)\n",
    "    return modified_list\n",
    "\n",
    "def remove_after_first_underscore(person_list):\n",
    "    modified_list = []\n",
    "    for person in person_list:\n",
    "        parts = person.split('_')\n",
    "        if len(parts) > 1:\n",
    "            modified_list.append(parts[0]) \n",
    "        else:\n",
    "            modified_list.append(person)\n",
    "    return modified_list\n",
    "\n",
    "def modify_person_column(current_df):\n",
    "    def process_person_column_2(person_str):\n",
    "        person_list = person_str.split(',')\n",
    "        modified_person_list = remove_after_second_underscore(person_list)\n",
    "        return str([f\"{p.strip()}\" for p in modified_person_list])\n",
    "\n",
    "    def process_person_column_1(person_str):\n",
    "        person_list = person_str.split(',')\n",
    "        modified_person_list = remove_after_first_underscore(person_list)\n",
    "        return ', '.join(modified_person_list)\n",
    "    \n",
    "    current_df['person'] = current_df['person'].apply(lambda x: process_person_column_2(x.strip(\"[]\").replace(\"'\", \"\")))\n",
    "    current_df['person_id'] = current_df['person_id'].apply(process_person_column_1)\n",
    "    return current_df\n",
    "\n",
    "\n",
    "def remove_values_from_list(lst, values):\n",
    "    return [item for item in lst if item not in values]\n",
    "\n",
    "\n",
    "def remove_values_from_string(s, values):\n",
    "    return ', '.join(item for item in s.split(', ') if item not in values)\n",
    "\n",
    "\n",
    "def list_to_string(lst):\n",
    "    return str(lst).replace('\"', '')\n",
    "\n",
    "def clean_data(df, values_to_remove, ids_to_remove):\n",
    "    df['person'] = df['person'].apply(lambda x: remove_values_from_list(x, values_to_remove))\n",
    "    df['person_id'] = df['person_id'].apply(lambda x: remove_values_from_string(x, ids_to_remove))\n",
    "    df['person'] = df['person'].apply(list_to_string)\n",
    "    return df\n",
    "\n",
    "\n",
    "if pengguna_user == '1':\n",
    "    values_to_remove = {'person_40'}\n",
    "    ids_to_remove = {'40'}\n",
    "else:\n",
    "    values_to_remove = {'person_999'}\n",
    "    ids_to_remove = {'999'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb100c3-d8b3-4e78-9bd1-cbf45e1e8f3a",
   "metadata": {},
   "source": [
    "This code is designed to clean and standardize the person and person_id columns in a dataset. It modifies these columns by removing unwanted parts after the first or second underscore in each entry, depending on the context. The functions remove_after_second_underscore and remove_after_first_underscore handle these modifications, while modify_person_column applies them to the DataFrame. Additionally, the clean_data function removes specific unwanted values (e.g., person_40 or person_999) from the columns based on the user context (pengguna_user). This ensures the dataset is properly formatted and cleaned of irrelevant or redundant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445619ff-89d4-4c94-bba0-4c47cef335c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'./face_dataset_{pengguna_user}_after_koreksi_sistem.csv')\n",
    "df_temp = df_correct_after.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc0e3a6a-686b-4bd0-994d-0c6a3481e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = modify_person_column(df_temp)\n",
    "df_cleaned['person'] = df_cleaned['person'].apply(literal_eval)\n",
    "df_cleaned = clean_data(df_cleaned, values_to_remove, ids_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab03b2-2452-4f02-ba8a-6e96b89bc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(f'./face_dataset_{pengguna_user}_after_koreksi_sistem_clean.csv', index=False)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc89b6-d39a-4a4c-80dd-0e76ac1177ed",
   "metadata": {},
   "source": [
    "To use this code:\n",
    "\n",
    "1. **Load the Dataset**: Read the dataset from `face_dataset_{pengguna_user}_after_koreksi_sistem.csv`.\n",
    "2. **Copy and Modify Data**: Create a temporary copy of the DataFrame, then apply `modify_person_column` to standardize and clean the `person` column.\n",
    "3. **Apply Additional Cleaning**: Convert the `person` column entries back from string to list format using `literal_eval`, then apply `clean_data` to remove specific unwanted values.\n",
    "4. **Save Cleaned Data**: Save the cleaned DataFrame to a new CSV file, `face_dataset{pengguna_user}_after_koreksi_sistem_clean.csv`.\n",
    "\n",
    "This process ensures the dataset is standardized and cleaned for accuracy and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421b63d-7684-49d5-aa7a-47129201542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def count_person_2_face(person_list_str):\n",
    "    person_list = ast.literal_eval(person_list_str)\n",
    "    return sum(bool(re.match(r'person_40_face_\\d+', person)) for person in person_list)\n",
    "\n",
    "total_count_before = df_correct_after['person'].apply(count_person_2_face).sum()\n",
    "total_count_after = df_cleaned['person'].apply(count_person_2_face).sum()\n",
    "\n",
    "print(f'Before: {total_count_before}')\n",
    "print(f'After: {total_count_after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3b0d8-d7b0-458e-9582-2a050592d267",
   "metadata": {},
   "source": [
    "This code calculates and compares the number of occurrences of `person_40_face_x` in the dataset before and after data cleaning.\n",
    "\n",
    "1. **Define Counting Function**: `count_person_2_face` parses the `person` column to count instances of `person_40_face_x` where `x` is a number.\n",
    "2. **Calculate Totals**: It computes the total counts of `person_40_face_x` entries in the dataset before (`df_correct_after`) and after (`df_cleaned`) cleaning.\n",
    "3. **Display Results**: The results are printed to show how many such instances were present before and after cleaning.\n",
    "\n",
    "**Example Output**:\n",
    "```\n",
    "Before: 25\n",
    "After: 10\n",
    "```\n",
    "\n",
    "This output shows that there were 25 occurrences of `person_40_face_x` before cleaning and 10 after, indicating the data has been successfully cleaned of unwanted entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d24f2e-a894-4932-8ec3-246c1c5b2919",
   "metadata": {},
   "source": [
    "#### ASSOCIATION RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6e1321-5d62-466f-80da-6927c0594ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    if isinstance(val, str):\n",
    "        return eval(val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299166a2-2476-459c-a1a3-018fc329c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_variable_names(df_list):\n",
    "    current_frame = inspect.currentframe()\n",
    "    caller_frame = current_frame.f_back\n",
    "    \n",
    "    variables = caller_frame.f_locals\n",
    "\n",
    "    data_names = [name for name, value in variables.items() if value in df_list]\n",
    "    \n",
    "    return data_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd6d26a7-39eb-4546-817a-115e2511e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_rules(df_list, min_support=0.03, min_threshold=0.9):\n",
    "    rules_dict = {}\n",
    "    \n",
    "    data_names = get_variable_names(df_list)\n",
    "    \n",
    "    if len(data_names) > 1:\n",
    "        for i, df in enumerate(df_list):\n",
    "            te = TransactionEncoder()\n",
    "            transactions = df['person'].apply(safe_eval).tolist()\n",
    "            te_ary = te.fit(transactions).transform(transactions)\n",
    "            df_transformed = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "            frequent_itemsets = apriori(df_transformed, min_support=min_support, use_colnames=True)\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "            rules['source'] = data_names[i]\n",
    "            rules_dict[i] = rules\n",
    "\n",
    "        for i in range(len(rules_dict) - 1):\n",
    "            for j in range(i + 1, len(rules_dict)):\n",
    "                common_rules = pd.merge(rules_dict[i], rules_dict[j], on=['antecedents', 'consequents'], suffixes=('_x', '_y'))\n",
    "                if not common_rules.empty:\n",
    "                    result = pd.DataFrame({\n",
    "                        'antecedents': common_rules['antecedents'],\n",
    "                        'consequents': common_rules['consequents'],\n",
    "                        'antecedent support': common_rules['antecedent support_x'],\n",
    "                        'consequent support': common_rules['consequent support_x'],\n",
    "                        'support': common_rules['support_x'],\n",
    "                        'confidence': common_rules['confidence_x'],\n",
    "                        'lift': common_rules['lift_x'],\n",
    "                    })\n",
    "                    result['sources'] = data_names[i] + ', ' + data_names[j]\n",
    "                    photo_id_j = df_list[j]['photo_id'].max()\n",
    "                    result['photo_id'] = str(photo_id_j)\n",
    "                    return result\n",
    "    else:\n",
    "        te = TransactionEncoder()\n",
    "        transactions = df_list[0]['person'].apply(safe_eval).tolist()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_transformed = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        frequent_itemsets = apriori(df_transformed, min_support=min_support, use_colnames=True)\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            'antecedents': rules['antecedents'],\n",
    "            'consequents': rules['consequents'],\n",
    "            'antecedent support': rules['antecedent support'],\n",
    "            'consequent support': rules['consequent support'],\n",
    "            'support': rules['support'],\n",
    "            'confidence': rules['confidence'],\n",
    "            'lift': rules['lift'],\n",
    "        })\n",
    "        result['sources'] = data_names[0]\n",
    "        result['photo_id'] = df_list[0]['photo_id'].max()\n",
    "        return result\n",
    "            \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3526173a-dff7-4576-a3b4-aae65fea3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_rules(rules_path):\n",
    "    if os.path.exists(rules_path):\n",
    "        rules_df = pd.read_csv(rules_path)\n",
    "        return rules_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_new_rules(rules_df, rules_path):\n",
    "    columns = [\n",
    "        'antecedents', 'consequents', 'antecedent support',\n",
    "        'consequent support', 'support', 'confidence', 'lift',\n",
    "        'sources', 'photo_id'\n",
    "    ]\n",
    "    \n",
    "    if rules_df is None:\n",
    "        rules_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    if not rules_df.empty:\n",
    "        rules_save = rules_df\n",
    "    else:\n",
    "        rules_save = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    rules_save.to_csv(rules_path, index=False)\n",
    "\n",
    "def check_rules_changes(rules_fix, existing_rules_df):\n",
    "    if rules_fix is not None:\n",
    "        if (rules_fix[['antecedents', 'consequents']].equals(existing_rules_df[['antecedents', 'consequents']])):\n",
    "            print(\"Ada foto tambahan, tetapi rules baru sama dengan rules lama.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Rules baru ditemukan dan disimpan. Menggunakan dataset dari {rules_fix['sources'].iloc[0]}.\")\n",
    "            return True\n",
    "    else:\n",
    "        print(\"Ada foto tambahan, tetapi rules baru sama dengan rules lama.\")\n",
    "        return False\n",
    "\n",
    "def check_and_update_rules(df, rules_path, min_support, min_threshold):\n",
    "    modified_df_clean = df\n",
    "\n",
    "    existing_rules_df = load_existing_rules(rules_path)\n",
    "\n",
    "    current_photo_count = modified_df_clean['photo_id'].nunique()\n",
    "\n",
    "    if existing_rules_df.empty:\n",
    "        previous_photo_count = 0\n",
    "    else:\n",
    "        previous_photo_count = existing_rules_df['photo_id'].max()\n",
    "\n",
    "    photo_count_increase = current_photo_count - previous_photo_count\n",
    "\n",
    "    percentage_increase = (photo_count_increase / previous_photo_count) * 100 if previous_photo_count > 0 else 100\n",
    "\n",
    "    if percentage_increase > 0.3:\n",
    "        data_1_4, _ = train_test_split(modified_df_clean, train_size=0.25, random_state=42)\n",
    "        data_3_4, _ = train_test_split(modified_df_clean, train_size=0.75, random_state=42)\n",
    "        data_1_2, _ = train_test_split(modified_df_clean, train_size=0.50, random_state=42)\n",
    "        data_full = modified_df_clean\n",
    "\n",
    "        \"\"\"\n",
    "        IN THIS SECTION YOU CAN CHOOSE, THE AMOUNT OF DATA YOU \n",
    "        WANT TO CREATE RULES, THE LESS REDUCE COMPUTATIONAL \n",
    "        WORKLOAD BUT CAUSE LESS VARIATION OF RULES, THE MORE \n",
    "        INCREASE COMPUTATIONAL WORKLOAD BUT CAUSE VARIATION OF RULES\n",
    "        \"\"\"\n",
    "        \n",
    "        # df_list = [data_1_4, data_3_4, data_1_2, data_full]\n",
    "        df_list = [data_full]\n",
    "\n",
    "        \"\"\"\n",
    "        YOU JUST HAVE TO UNCOMMENT THE UNNECESSARY PARTS OF ONE OF THE Df_list VARIABLES\n",
    "        \"\"\"\n",
    "        \n",
    "        rules_fix = find_common_rules(df_list, min_support=min_support, min_threshold=min_threshold)\n",
    "\n",
    "        rules_fix.to_csv('./rules_temp.csv', index=False)\n",
    "        if existing_rules_df.empty:\n",
    "            print(\"Rules baru telah dibuat\")\n",
    "            save_new_rules(rules_fix, rules_path)\n",
    "            return rules_fix, modified_df_clean\n",
    "        else:\n",
    "            rules_fix = pd.read_csv('./rules_temp.csv')\n",
    "            if check_rules_changes(rules_fix, existing_rules_df):\n",
    "                save_new_rules(rules_fix, rules_path)\n",
    "                return rules_fix, modified_df_clean\n",
    "            else:\n",
    "                existing_rules_df[\"photo_id\"] = rules_fix[\"photo_id\"].iloc[0]\n",
    "                return existing_rules_df, modified_df_clean\n",
    "    else:\n",
    "        print(\"Menggunakan rules lama. Tidak ada perubahan signifikan dalam jumlah foto.\")\n",
    "        return existing_rules_df, modified_df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc2c3b-db80-42b4-ba89-42498dbc840b",
   "metadata": {},
   "source": [
    "This script is designed to manage and update rules based on data changes. It starts by loading existing rules and checking for new data. If the number of photos increases significantly, it splits the dataset into various parts and generates new rules using the Apriori algorithm. The function `find_common_rules` creates these rules and compares them with the existing ones to determine if they have changed. The updated rules are then saved, and appropriate messages are printed to indicate whether new rules were added or if the old rules remain valid. This process ensures that the rules stay current with the data while managing computational workload efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e3287-c517-43f4-a9c4-9fbabf5b815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(f'./face_dataset_{pengguna_user}_after_koreksi_sistem_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6826a-7674-4bc0-ba69-421935150610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "support_rules = 0.03\n",
    "confidence_rules = 0.7\n",
    "path_rules = f'./rules_file/{pengguna_user}/filtered_rules_{pengguna_user}_{support_rules}_{confidence_rules}_test.csv'\n",
    "rules_df, data_photo = check_and_update_rules(df_cleaned, rules_path=path_rules, min_support=support_rules, min_threshold=confidence_rules)\n",
    "filtered_rules = rules_df\n",
    "print(f\"Rules yang dipilih berasal dari {rules_df[\"sources\"].iloc[0]}  membuat sebanyak {len(filtered_rules)} rules\")\n",
    "filtered_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369efaf-649b-477e-8b9f-f49a54cfca20",
   "metadata": {},
   "source": [
    "To use this script:\n",
    "\n",
    "1. **Load Cleaned Data**: Read the cleaned face dataset from the specified CSV file.\n",
    "2. **Set Parameters**: Define `support_rules` and `confidence_rules` for filtering rules, and specify the `path_rules` for saving the updated rules.\n",
    "3. **Update Rules**: Call `check_and_update_rules()` to check for significant changes in the dataset, generate or update rules, and save them to the specified path.\n",
    "4. **View Results**: Print the source of the rules and the number of rules created.\n",
    "\n",
    "Example: This script will update rules based on the provided dataset and parameters, saving the results and displaying how many new rules were generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5599d-92c1-42a0-8a5a-e94b7f511d2d",
   "metadata": {},
   "source": [
    "### CLASSIFICATION OF INDIVIDUAL FACIAL CORRELATION BETWEEN PHOTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff79a9d5-7bf2-48c9-bbf7-36e180225fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rule(antecedents, consequents):\n",
    "    \"\"\" Normalize the rule to a canonical form. \"\"\"\n",
    "    return (frozenset(antecedents), frozenset(consequents))\n",
    "\n",
    "def group_correlated_rules(rules_df):\n",
    "    grouped_rules = []\n",
    "    used_indices = set()\n",
    "    rule_map = {}\n",
    "\n",
    "    for i, row_i in rules_df.iterrows():\n",
    "        if i in used_indices:\n",
    "            continue\n",
    "        \n",
    "        new_antecedents = set(row_i['antecedents'])\n",
    "        new_consequents = set(row_i['consequents'])\n",
    "\n",
    "        for j, row_j in rules_df.iterrows():\n",
    "\n",
    "            if i == j or j in used_indices:\n",
    "                continue\n",
    "\n",
    "            if not new_antecedents.isdisjoint(row_j['antecedents']) and not new_consequents.isdisjoint(row_j['consequents']):\n",
    "                new_antecedents.update(row_j['antecedents'])\n",
    "                new_consequents.update(row_j['consequents'])\n",
    "                used_indices.add(j)\n",
    "        \n",
    "        # Normalize the rule\n",
    "        rule = normalize_rule(new_antecedents, new_consequents)\n",
    "        grouped_rules.append(rule)\n",
    "        used_indices.add(i)\n",
    "\n",
    "    final_rules = []\n",
    "    seen_rules = set()\n",
    "\n",
    "    for rule in grouped_rules:\n",
    "        antecedents, consequents = rule\n",
    "        inverse_rule = normalize_rule(consequents, antecedents)\n",
    "        \n",
    "        if inverse_rule not in seen_rules:\n",
    "            final_rules.append(rule)\n",
    "            seen_rules.add(rule)\n",
    "    \n",
    "    return final_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6f9fda-5935-4961-8276-be8803428fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules(row):\n",
    "    persons = set(row['person'])\n",
    "    albums = set().\n",
    "    for idx, (antecedents, consequents) in enumerate(grouped_rules):\n",
    "        for antecedent in antecedents:\n",
    "            for consequent in consequents:\n",
    "                if antecedent != consequent and consequent in persons and antecedent in persons:\n",
    "                    albums.add(idx)\n",
    "                    break \n",
    "    \n",
    "    return list(albums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9d4ce-6ba2-4fae-8c7d-7b43100e10ae",
   "metadata": {},
   "source": [
    "The provided code performs the following tasks:\n",
    "\n",
    "1. **Normalize Rules**: `normalize_rule` converts rules into a canonical form by using `frozenset` for both antecedents and consequents.\n",
    "2. **Group Correlated Rules**: `group_correlated_rules` groups similar rules by merging antecedents and consequents of correlated rules and removes duplicates and inverse rules.\n",
    "3. **Apply Rules**: `apply_rules` checks each row of the dataset to see if it matches any grouped rules, assigning relevant albums based on rule correlations.\n",
    "\n",
    "In summary, the script normalizes and groups correlated rules, removes redundant and inverse rules, and applies these rules to classify data entries, thereby facilitating more effective rule-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fddf1d-d24f-40e9-8027-e56ad2b78711",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_photo = pd.read_csv(f'./face_dataset_{pengguna_user}_after_koreksi_sistem_clean.csv')\n",
    "df_photos = data_photodf_photos['person'] = df_photos['person'].apply(safe_eval)\n",
    "filtered_rules['antecedents'] = filtered_rules['antecedents'].apply(safe_eval)\n",
    "filtered_rules['consequents'] = filtered_rules['consequents'].apply(safe_eval)\n",
    "grouped_rules = group_correlated_rules(filtered_rules)\n",
    "grup_rule = pd.DataFrame(grouped_rules, columns=['IF', 'THEN'])\n",
    "grup_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c31eb5bb-dece-471e-af43-0c3150d81871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create albums\n",
    "df_photos['albums'] = df_photos.apply(apply_rules, axis=1)\n",
    "df_photos.albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3fc33-0ae1-4442-bb49-93a60a5bb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29764a-af05-4145-8528-c8490454538f",
   "metadata": {},
   "source": [
    "Hereâ€™s a concise explanation for using the provided code:\n",
    "\n",
    "1. **Load and Prepare Data**: Read the cleaned photo dataset and apply `safe_eval` to process the 'person' column, and similarly process the antecedents and consequents of the filtered rules.\n",
    "\n",
    "2. **Group Correlated Rules**: Use `group_correlated_rules` to normalize and group the rules, removing duplicates and inverse rules.\n",
    "\n",
    "3. **Create Albums**: Apply the grouped rules to classify the photos into albums using the `apply_rules` function, which assigns relevant album indices based on rule matches.\n",
    "\n",
    "In summary, this script loads and processes photo data, groups correlated rules, and classifies photos into albums according to these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abbbb1-4729-4c78-b40d-01ffbc4e9e19",
   "metadata": {},
   "source": [
    "### GROUPING PHOTOS BASED ON CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb35c0ef-05c5-476c-a1d1-10b04f3160f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(list1, list2):\n",
    "    counter1 = Counter(list1)\n",
    "    counter2 = Counter(list2)\n",
    "    common_elements = sum((counter1 & counter2).values())\n",
    "    total_elements = max(len(list1), len(list2))\n",
    "    similarity = common_elements / total_elements\n",
    "    return similarity\n",
    "\n",
    "def cleanse_album_df(album_df):\n",
    "    rows_to_drop = set()\n",
    "    for (idx1, row1), (idx2, row2) in combinations(album_df.iterrows(), 2):\n",
    "        album_similarity = calculate_similarity(row1['album'], row2['album'])\n",
    "        photo_id_similarity = calculate_similarity(row1['photo_id'], row2['photo_id'])\n",
    "\n",
    "        \"\"\"\n",
    "        IN THIS SECTION, PART OF BRACHING \"IF\" YOU CAN CHANGE THE THRESHOLD, BECAUSE IN THIS PART IT IS A TRIAL AND ERROR EXPERIMENT\n",
    "        \"\"\"\n",
    "        if album_similarity > 0.3:\n",
    "            if len(row1['photo_id']) > len(row2['photo_id']):\n",
    "                larger_idx, smaller_idx = idx1, idx2\n",
    "            else:\n",
    "                larger_idx, smaller_idx = idx2, idx1\n",
    "\n",
    "            larger_album_id = album_df.loc[larger_idx, 'photo_id']\n",
    "            smaller_album_id = album_df.loc[smaller_idx, 'photo_id']\n",
    "\n",
    "            larger_album = album_df.loc[larger_idx, 'album']\n",
    "            smaller_album = album_df.loc[smaller_idx, 'album']\n",
    "            \n",
    "            unique_photo_ids = set(smaller_album_id) - set(larger_album_id)\n",
    "            album_df.at[larger_idx, 'photo_id'] = list(set(larger_album_id) | unique_photo_ids)\n",
    "\n",
    "            unique_photo = set(smaller_album) - set(larger_album)\n",
    "            album_df.at[larger_idx, 'album'] = list(set(larger_album) | unique_photo)\n",
    "            \n",
    "            rows_to_drop.add(smaller_idx)\n",
    "    \n",
    "    cleansed_df = album_df.drop(index=rows_to_drop).reset_index(drop=True)\n",
    "    return cleansed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e02b1-ddc1-4179-814e-91d4b092c2b1",
   "metadata": {},
   "source": [
    "The provided code defines two functions: `calculate_similarity` and `cleanse_album_df`. The `calculate_similarity` function measures the similarity between two lists by comparing their common elements relative to the larger list. The `cleanse_album_df` function uses this similarity measure to clean an album DataFrame. It iterates through pairs of rows, and if the similarity of their 'album' and 'photo_id' values exceeds 0.3, it merges the albums, consolidating photo IDs and album entries into the larger album and marking the smaller album for deletion. The final output is a cleaned DataFrame with redundant albums removed and their contents consolidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6a5ce-4eba-40ae-9005-1b18114e3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_dict = {}\n",
    "for _, row in df_photos.iterrows():\n",
    "    album_ids = row['albums']\n",
    "    for album_id in album_ids:\n",
    "        if album_id not in album_dict:\n",
    "            album_dict[album_id] = {'persons': set(), 'photo_ids': []}\n",
    "        album_dict[album_id]['photo_ids'].append(row['photo_id'])\n",
    "        album_dict[album_id]['persons'].update(row['person'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae190cca-8878-4e6a-9f32-1505c506d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_album = {\n",
    "    'album_id': [],\n",
    "    'album': [],\n",
    "    'photo_id': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3705af4-dbf0-48d2-a2e2-ecd6bc60384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for album_id, info in album_dict.items():\n",
    "\n",
    "    data_album['album_id'].append(album_id)\n",
    "    data_album['album'].append(list(info['persons']))\n",
    "    data_album['photo_id'].append(info['photo_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c31d93-6be4-4a76-a217-4fc095052360",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df = pd.DataFrame(data_album)\n",
    "cleansed_album_df = cleanse_album_df(album_df)\n",
    "cleansed_album_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4d55d-e853-49ee-9be0-d4147499e46c",
   "metadata": {},
   "source": [
    "The provided code performs the following steps:\n",
    "\n",
    "1. **Create Album Dictionary**: It iterates over each row in `df_photos` to build a dictionary (`album_dict`). Each key is an album ID, and the value contains a set of persons and a list of photo IDs associated with that album.\n",
    "\n",
    "2. **Prepare Data for DataFrame**: It prepares data for a new DataFrame (`album_df`) by extracting album IDs, associated persons, and photo IDs from the dictionary.\n",
    "\n",
    "3. **Create and Cleanse DataFrame**: It creates `album_df` from the prepared data and then applies the `cleanse_album_df` function to remove redundant albums and consolidate photo IDs.\n",
    "\n",
    "The final output is `cleansed_album_df`, which contains the cleaned and consolidated album data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a2dbf-977a-4503-b522-43b8fcd0cf88",
   "metadata": {},
   "source": [
    "### ALBUM FORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d60862c1-b8ce-4b09-8484-a597f309acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_album(album_id, photos, df, pengguna_user):\n",
    " \n",
    "    hasil_path = f'./result gallery {pengguna_user}'\n",
    "    album_path = f'{hasil_path}/{album_id}'\n",
    "    os.makedirs(album_path, exist_ok=True)\n",
    "    \n",
    "    for photo_id in photos:\n",
    "\n",
    "        photo_name = df.loc[df['photo_id'] == photo_id, 'photo_name'].values\n",
    "        if len(photo_name) > 0:\n",
    "            photo_name = photo_name[0]\n",
    "            photo_path = f'./{pengguna_user}/{photo_name}'\n",
    "            shutil.copy(photo_path, f'{album_path}/{photo_name}')\n",
    "    \n",
    "    return album_path\n",
    "\n",
    "def process_photos(df, pengguna_user):\n",
    "\n",
    "    hasil_path = f'./result gallery {pengguna_user}'\n",
    "    os.makedirs(hasil_path, exist_ok=True)\n",
    "\n",
    "    all_photo_ids = df['photo_id'].unique()\n",
    "    album_photo_ids = [photo_id for ids in cleansed_album_df['photo_id'] for photo_id in ids]\n",
    "\n",
    "    for photo_id in all_photo_ids:\n",
    "        if photo_id not in album_photo_ids:\n",
    "            photo_name = df.loc[df['photo_id'] == photo_id, 'photo_name'].values[0]\n",
    "            photo_path = f'./{pengguna_user}/{photo_name}'\n",
    "            shutil.copy(photo_path, f'{hasil_path}/{photo_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651526d-cb46-42cc-a232-1d48765c882c",
   "metadata": {},
   "source": [
    "The code provides functions to organize and copy photos into albums. The `create_album` function creates a directory for each album and copies photos into it based on their IDs from a DataFrame. The `process_photos` function handles photos not included in any album by copying them into a separate directory for the user. Both functions use paths constructed with the user's identifier and ensure that the necessary directories exist before copying files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27585d0-f652-4ba3-91bb-1ff2379527ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "albums = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "process_photos(df_photos, pengguna_user)\n",
    "\n",
    "for _, row in cleansed_album_df.iterrows():\n",
    "    album_id = row['album_id']\n",
    "    photos = row['photo_id']\n",
    "    album_path = create_album(album_id, photos, df_photos, pengguna_user)\n",
    "    albums.append(album_path)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Album successfully created in:\", albums)\n",
    "print(f\"Computation Time: {end_time - start_time:.2f} second\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d3e99-cd5e-4377-86e3-b24b9cb2e86b",
   "metadata": {},
   "source": [
    "The code snippet processes photos by first executing `process_photos` to handle images not included in any album. Then, it iterates through the `cleansed_album_df` DataFrame to create individual albums using `create_album`, which organizes photos into respective directories. The paths of these created albums are collected in the `albums` list. Finally, the script prints the locations of the created albums and the total computation time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
